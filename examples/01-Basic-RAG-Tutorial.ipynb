{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c5123e",
   "metadata": {},
   "source": [
    "Content sourced from https://www.langchain.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bd7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a6220031-9005-4a4f-ba52-e925aacfdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1441017-96c8-4629-a0c2-a38076ef58c8",
   "metadata": {},
   "source": [
    "# Langchain Basics\n",
    "\n",
    "## Chains\n",
    "https://python.langchain.com/docs/modules/chains\n",
    "\n",
    "Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LangChain Expression Language (LCEL). The components of the chain are executed sequentially, with the output of each component acting as the input to the next component. The following contains a simple chain where we define a question/prompt, give the prompt to the model, and parse the model's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2f6eda6-ae96-4980-9098-fb604561b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT YOUR OPENAI API KEY: https://platform.openai.com/docs/models , make an account\n",
    "os.environ[\"OPENAI_API_KEY\"] = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af2b26a5-5afb-4668-9631-75465108dd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRobot: Why did the ice cream go to therapy? Because it was feeling a little rocky road.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chatprompttemplate: https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\")\n",
    "# using OpenAI's pre-trained LLM\n",
    "model = OpenAI()\n",
    "# converts the LLM output into something humans can understand\n",
    "# stroutputparserL https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.string.StrOutputParser.html\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# chains are constructed with the pipe operator: '|'\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "# invoke executes the chain\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb6a3f-47ee-4b4c-a409-dc6aa6cc3de8",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation \n",
    "https://python.langchain.com/docs/use_cases/question_answering/\n",
    "https://python.langchain.com/docs/use_cases/question_answering/quickstart\n",
    "\n",
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "In the code below, two methods are shown on how to incporate RAG with OpenAI's LLM by training the model on the following web article: 'LLM Powered Autonomous Agents' by Lilian Weng. Before a model can retrieve data or generate responses, the new data must be indexed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9bc52-b820-4f82-a4cf-7b5b6e527d0c",
   "metadata": {},
   "source": [
    "### 1. Indexing\n",
    "![Indexing](useful_figures/rag_indexing.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5582ac7c-fadf-45f9-8441-996fcee35f97",
   "metadata": {},
   "source": [
    "#### 1A. Indexing: Load\n",
    "\n",
    "We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict).\n",
    "\n",
    "In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others.\n",
    "\n",
    "For more info on types of document loaders that LangChain offers: https://python.langchain.com/docs/modules/data_connection/document_loaders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a954476-5213-4888-8ea0-52c03478afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beautiful soup is a package for pulling data out of HTML & XML files: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "# only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb7522f5-b921-4bf4-89f1-ba482fd8f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "# We can view the contents of the document object \n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb3b8c-cd88-46fb-ad5f-6fb5477be2f7",
   "metadata": {},
   "source": [
    "#### 1B. Indexing: Split\n",
    "Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set add_start_index=True so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5373b182-bc20-4003-b826-38b3e06b42a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "# here we are splitting our document object into smaller chunks\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "06799ac6-f3ab-4e59-aff8-0c86acb83e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 , 969\n"
     ]
    }
   ],
   "source": [
    "print(len(all_splits), ',', len(all_splits[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d652c1b-b1ab-46a9-b69b-be2f219d98e0",
   "metadata": {},
   "source": [
    "#### 1C. Indexing: Store\n",
    "Now we need to index our 66 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "We can embed and store all of our document splits in a single command using the Chroma vector store and OpenAIEmbeddings model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c3fa8791-88be-4e9b-bfe1-506b123ca4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d902f54-087e-45c9-942a-08f0717913eb",
   "metadata": {},
   "source": [
    "### 2. Retrieval and Generation\n",
    "![Retrieval](useful_figures/rag_retrieval_generation.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ec3fa-9e48-431c-8d8c-8ad163437de3",
   "metadata": {},
   "source": [
    "#### 2A: Retrieval\n",
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "First we need to define our logic for searching over documents. LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "\n",
    "The most common type of Retriever is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facilitate retrieval. Any VectorStore can easily be turned into a Retriever with VectorStore.as_retriever():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d4031e66-35a5-4dec-9ccc-fdcb360e35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ef5279a-2ca4-49c0-ba51-46c91909fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the retriever is searching for chunks which seem relevant to the prompt\n",
    "# note: retriever.invoke means that the retreiver is a chain itself\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da2ca510-b13f-47a4-8c1d-cade83614006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output below shows that 6 chunks were relevant to the question\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e5cca292-c158-4622-914d-d4604da5ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72237c1e-5186-47af-8d9c-e033a5ce02c1",
   "metadata": {},
   "source": [
    "#### 2A: Generate\n",
    "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
    "\n",
    "We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5036ed91-1a3b-47db-ac76-6fe80fc7f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e5ec645-94ce-4816-bd97-ecc889bd1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag-prompt: https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=bf831fe5-56d8-572d-a9ca-fea6f3d0f30e\n",
    "# here we are using a pre-made prompt from langchain\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b192673-49a2-4360-a348-ba6f88ff954d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the prompt we pulled contains {context} and {question} for us to customize\n",
    "# .tomessages converts the output from a langchain prompt value object to a list\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e779c4-0b65-4bf8-bbd5-085cca13a4cd",
   "metadata": {},
   "source": [
    "We’ll use the LCEL Runnable protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box.\n",
    "\n",
    "RunnablePassthrough: RunnablePassthrough allows to pass inputs unchanged or with the addition of extra keys. This typically is used in conjuction with RunnableParallel to assign data to a new key in the map. RunnablePassthrough() called on it’s own, will simply take the input and pass it through. (https://python.langchain.com/docs/expression_language/how_to/passthrough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "214704b2-ae2f-4517-94cf-17941fb88c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will be useful to join together the relevant chunks we retrieved earlier\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# now we can construct our chain using all of the components we defined\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "badfc667-4182-493f-90df-527251326ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This process involves transforming big tasks into multiple manageable tasks to enhance model performance. It can be done through simple prompting, task-specific instructions, or with human inputs."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53b724-b13c-449c-bb39-e8f3ef4be015",
   "metadata": {},
   "source": [
    "### Customizing the prompt\n",
    "As shown above, we can load prompts (e.g., this RAG prompt) from the prompt hub. The prompt can also be easily customized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3894f55a-acdb-40c5-8f53-31c2b443a46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "Use three sentences maximum and keep the answer as concise as possible.\n",
      "Always say \"thanks for asking!\" at the end of the answer.\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{context}\u001b[0m\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "custom_rag_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a66543a4-6989-4962-93f1-40cce578660f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down complex tasks into smaller and simpler steps to make them more manageable. This can be done using techniques like Chain of Thought or Tree of Thoughts to guide the model in decomposing hard tasks effectively. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs. Thanks for asking!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73370408-9e78-4d7e-b999-3f9494b6bc3c",
   "metadata": {},
   "source": [
    "### Using Chains to to query databases\n",
    "https://python.langchain.com/docs/use_cases/sql/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "567b5846-b42d-477e-955d-30f6f3a239b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653c8a38-9fe0-46f4-83f8-7ce64dd02ce6",
   "metadata": {},
   "source": [
    "To run the following code, you must have the file Chinook.db in the same folder as this Jupyter notebook. Run the following line by line:\n",
    "\n",
    "curl -O https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\n",
    "\n",
    "sqlite3 Chinook.db\n",
    "\n",
    ".read Chinook_Sqlite.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7244c80f-7a4a-49c8-8852-2a28a915d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqlite\n",
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Antônio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')]\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "db.run(\"SELECT * FROM Artist LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fa6a6-f4ab-4d13-89a1-42281122abde",
   "metadata": {},
   "source": [
    "Let’s create a simple chain that takes a question, turns it into a SQL query, executes the query, and uses the result to answer the original question.\n",
    "\n",
    "Convert question to SQL query\n",
    "The first step in a SQL chain or agent is to take the user input and convert it to a SQL query. LangChain comes with a built-in chain for this: create_sql_query_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6d0ad80-9063-42c4-b604-5a7ef8b84042",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fcd7881-7185-408d-8672-720e8317be46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT COUNT(\"EmployeeId\") AS \"TotalEmployees\" FROM \"Employee\"'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = create_sql_query_chain(llm, db)\n",
    "response = chain.invoke({\"question\": \"How many employees are there\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2fd8790c-69a3-4594-88e6-d4d56affe17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(8,)]'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2fafa-7852-40bd-bf92-d09f7206b53b",
   "metadata": {},
   "source": [
    "Execute SQL query\n",
    "Now that we’ve generated a SQL query, we’ll want to execute it. This is the most dangerous part of creating a SQL chain. Consider carefully if it is OK to run automated queries over your data. Minimize the database connection permissions as much as possible. Consider adding a human approval step to you chains before query execution (see below).\n",
    "\n",
    "We can use the QuerySQLDatabaseTool to easily add query execution to our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "66ea5df2-ac96-413e-a411-e6beff48cdb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(8,)]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_query = QuerySQLDataBaseTool(db=db)\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "chain = write_query | execute_query\n",
    "chain.invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd973c-acba-4caa-bd1b-9c6d81c5c4b6",
   "metadata": {},
   "source": [
    "#### Answer the question\n",
    "Now that we’ve got a way to automatically generate and execute queries, we just need to combine the original question and SQL query result to generate a final answer. We can do this by passing question and result to the LLM once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2849e11-9764-4509-8700-72da7fac3a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a total of 8 employees.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "answer = answer_prompt | llm | StrOutputParser()\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(\n",
    "        result=itemgetter(\"query\") | execute_query\n",
    "    )\n",
    "    | answer\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How many employees are there\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99b42d-a725-4f18-8411-5cbd41420333",
   "metadata": {},
   "source": [
    "## Related Topics\n",
    "- Retain conversations with a chat history: https://python.langchain.com/docs/use_cases/question_answering/chat_history\n",
    "\n",
    "- Using a locally hosted LLM for RAG: https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
    "\n",
    "- Streaming intermediate steps: https://python.langchain.com/docs/use_cases/question_answering/streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
